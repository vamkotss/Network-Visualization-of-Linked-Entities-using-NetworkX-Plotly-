{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6510300-828c-4f5f-9fdb-11585f8cf9bd",
   "metadata": {},
   "source": [
    "Network_Visualization_SriVamsi_Kota"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f121351b-f652-40ee-aa0e-d6b9e1b2ff7e",
   "metadata": {},
   "source": [
    "### Install Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91369af7-e250-4b0a-94bc-b510946ea9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.51 (from langchain)\n",
      "  Downloading langchain_core-0.3.51-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.28-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.40-cp313-cp313-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.51->langchain)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.51->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.13.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.16-cp313-cp313-win_amd64.whl.metadata (42 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp313-cp313-win_amd64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.33.1-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.1.1-cp313-cp313-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Downloading langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 6.8 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.3.51-py3-none-any.whl (423 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.3.28-py3-none-any.whl (357 kB)\n",
      "Downloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.1-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 11.5 MB/s eta 0:00:00\n",
      "Downloading sqlalchemy-2.0.40-cp313-cp313-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 10.6 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading greenlet-3.1.1-cp313-cp313-win_amd64.whl (299 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading orjson-3.10.16-cp313-cp313-win_amd64.whl (133 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading zstandard-0.23.0-cp313-cp313-win_amd64.whl (495 kB)\n",
      "Installing collected packages: zstandard, typing-inspection, tenacity, pydantic-core, orjson, jsonpatch, greenlet, annotated-types, SQLAlchemy, requests-toolbelt, pydantic, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed SQLAlchemy-2.0.40 annotated-types-0.7.0 greenlet-3.1.1 jsonpatch-1.33 langchain-0.3.23 langchain-core-0.3.51 langchain-text-splitters-0.3.8 langsmith-0.3.28 orjson-3.10.16 pydantic-2.11.3 pydantic-core-2.33.1 requests-toolbelt-1.0.0 tenacity-9.1.2 typing-inspection-0.4.0 zstandard-0.23.0\n",
      "Requirement already satisfied: langchain-core in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.51)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (0.3.28)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (4.13.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (2.11.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (3.10.16)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (1.3.1)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community) (0.3.51)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community) (0.3.23)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community) (6.0.2)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Downloading aiohttp-3.11.16-cp313-cp313-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community) (9.1.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community) (0.3.28)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting numpy<3,>=1.26.2 (from langchain-community)\n",
      "  Downloading numpy-2.2.4-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading frozenlist-1.5.0-cp313-cp313-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading multidict-6.4.2-cp313-cp313-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading propcache-0.3.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading yarl-1.19.0-cp313-cp313-win_amd64.whl.metadata (74 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (2.11.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (4.13.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.4.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 1.8/2.5 MB 14.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 12.8 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.11.16-cp313-cp313-win_amd64.whl (436 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading numpy-2.2.4-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.6/12.6 MB 14.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.5/12.6 MB 14.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.4/12.6 MB 13.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.3/12.6 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 13.3 MB/s eta 0:00:00\n",
      "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp313-cp313-win_amd64.whl (51 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.4.2-cp313-cp313-win_amd64.whl (38 kB)\n",
      "Downloading propcache-0.3.1-cp313-cp313-win_amd64.whl (44 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.19.0-cp313-cp313-win_amd64.whl (92 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: python-dotenv, propcache, numpy, mypy-extensions, multidict, marshmallow, httpx-sse, frozenlist, aiohappyeyeballs, yarl, typing-inspect, aiosignal, pydantic-settings, dataclasses-json, aiohttp, langchain-community\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 dataclasses-json-0.6.7 frozenlist-1.5.0 httpx-sse-0.4.0 langchain-community-0.3.21 marshmallow-3.26.1 multidict-6.4.2 mypy-extensions-1.0.0 numpy-2.2.4 propcache-0.3.1 pydantic-settings-2.8.1 python-dotenv-1.1.0 typing-inspect-0.9.0 yarl-1.19.0\n",
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.166.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Downloading google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-6.30.2-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pydantic in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (2.11.3)\n",
      "Collecting tqdm (from google-generativeai)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (4.13.1)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.69.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic->google-generativeai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic->google-generativeai) (0.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio-1.72.0rc1-cp313-cp313-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.72.0rc1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading google_generativeai-0.8.4-py3-none-any.whl (175 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.3/1.3 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 6.4 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading google_api_python_client-2.166.0-py2.py3-none-any.whl (13.2 MB)\n",
      "   ---------------------------------------- 0.0/13.2 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.6/13.2 MB 13.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.5/13.2 MB 13.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.7/13.2 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.5/13.2 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.2/13.2 MB 13.2 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.69.2-py3-none-any.whl (293 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio-1.72.0rc1-cp313-cp313-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 2.9/4.3 MB 14.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 12.6 MB/s eta 0:00:00\n",
      "Downloading grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: uritemplate, tqdm, pyparsing, pyasn1, protobuf, grpcio, cachetools, rsa, pyasn1-modules, proto-plus, httplib2, googleapis-common-protos, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "Successfully installed cachetools-5.5.2 google-ai-generativelanguage-0.6.15 google-api-core-2.24.2 google-api-python-client-2.166.0 google-auth-2.38.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.4 googleapis-common-protos-1.69.2 grpcio-1.72.0rc1 grpcio-status-1.71.0 httplib2-0.22.0 proto-plus-1.26.1 protobuf-5.29.4 pyasn1-0.6.1 pyasn1-modules-0.4.2 pyparsing-3.2.3 rsa-4.9 tqdm-4.67.1 uritemplate-4.1.1\n",
      "Collecting gephistreamer\n",
      "  Downloading GephiStreamer-2.0.3.zip (8.2 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: requests in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gephistreamer) (2.32.3)\n",
      "Collecting ws4py (from gephistreamer)\n",
      "  Downloading ws4py-0.6.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting enum34 (from gephistreamer)\n",
      "  Downloading enum34-1.1.10-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->gephistreamer) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->gephistreamer) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->gephistreamer) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sriva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->gephistreamer) (2025.1.31)\n",
      "Downloading enum34-1.1.10-py3-none-any.whl (11 kB)\n",
      "Downloading ws4py-0.6.0-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: gephistreamer\n",
      "  Building wheel for gephistreamer (pyproject.toml): started\n",
      "  Building wheel for gephistreamer (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gephistreamer: filename=gephistreamer-2.0.3-py3-none-any.whl size=4853 sha256=4fff44c9affa93424c6e2f1f9f8c39864b02dd41aa8bd331d9af655391aeaded\n",
      "  Stored in directory: c:\\users\\sriva\\appdata\\local\\pip\\cache\\wheels\\7a\\61\\c0\\fa2a5f09d6f798f1104f0eede55d9993abe6db52683d518cf6\n",
      "Successfully built gephistreamer\n",
      "Installing collected packages: ws4py, enum34, gephistreamer\n",
      "Successfully installed enum34-1.1.10 gephistreamer-2.0.3 ws4py-0.6.0\n"
     ]
    }
   ],
   "source": [
    "#! pip install langchain\n",
    "#! pip install langchain-core\n",
    "#! pip install langchain-community\n",
    "#! pip install google-generativeai\n",
    "#! pip install gephistreamer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5578a4-7904-4304-b2a7-455e4a0bb647",
   "metadata": {},
   "source": [
    "### Create a list of Web URL for data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ebb040-ebd8-429e-b5f1-0a2f21723c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of top Tech leaders\n",
    "url_list=[  \n",
    "    'https://en.wikipedia.org/wiki/Elon_Musk',\n",
    "    'https://en.wikipedia.org/wiki/Mark_Zuckerberg',\n",
    "    'https://en.wikipedia.org/wiki/Bill_Gates',\n",
    "    'https://en.wikipedia.org/wiki/Jeff_Bezos',\n",
    "    'https://en.wikipedia.org/wiki/Steve_Jobs',\n",
    "    'https://en.wikipedia.org/wiki/Sam_Altman',\n",
    "    'https://en.wikipedia.org/wiki/Larry_Ellison',\n",
    "    'https://en.wikipedia.org/wiki/Larry_Page',\n",
    "    'https://en.wikipedia.org/wiki/Sundar_Pichai',\n",
    "    'https://en.wikipedia.org/wiki/Satya_Nadella'  \n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ca2a96-ddaf-4525-9655-613d18df5e24",
   "metadata": {},
   "source": [
    "### Define function to clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba66094-5f63-49d4-ae21-65b056f71a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to clean the extracted web URL data\n",
    "import re #for regular expression \n",
    " \n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]*?>', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    # Trim leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a586d1a-8713-4f9f-a506-10f4774d7093",
   "metadata": {},
   "source": [
    "### Use Langchain framework to extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f8a0d43-be8a-4f37-967d-fe26a304ccc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# extract the data from the URLs\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    " \n",
    "def extract_data_from_URL(url):\n",
    "    loader=WebBaseLoader([url])\n",
    "    data=loader.load().pop().page_content\n",
    "    data=clean_text(data)\n",
    "    documents=[Document(page_content=data)]\n",
    "    # print(documents)\n",
    "    splitter=RecursiveCharacterTextSplitter(chunk_size=3000,chunk_overlap=100)\n",
    "    smaller_doc=splitter.split_documents(documents)\n",
    "    print(len(smaller_doc))\n",
    "    return smaller_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6d7cc7-6e2f-478e-bd6d-bafb08330543",
   "metadata": {},
   "source": [
    "### Use Gemini API and generate sample response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a526eace-5b1d-4603-ba9c-4b30cfd1fd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"title\": \"Texas Tech Surge: A 2040 Vision\", \"story\": \"By 2040, the Texas tech scene had exploded, defying all expectations.  It wasn't a single event, but a confluence of factors.  First, the state aggressively invested in STEM education, creating a pipeline of highly skilled graduates from universities across the state, not just Austin.  Smaller cities like San Antonio, El Paso, and even Lubbock saw flourishing tech incubators, supported by state grants and a renewed focus on vocational training.  Secondly, the regulatory environment shifted.  Texas streamlined its permitting processes, making it easier for startups to establish themselves and scale rapidly.  Red tape was replaced with red carpet welcomes.  Thirdly, a concerted effort was made to diversify the tech workforce.  Initiatives focused on attracting and retaining talent from diverse backgrounds, creating a vibrant and inclusive ecosystem.  This included generous scholarship programs targeted at underrepresented minorities and significant investment in affordable housing near tech hubs.  Fourth, a strategic focus on specific sectors like renewable energy and aerospace technology capitalized on Texas' existing strengths.  Austin remained a powerhouse, but it was no longer the sole player.  Smaller cities blossomed, each focusing on niche areas.  San Antonio became a leader in biomed tech, while El Paso emerged as a hub for advanced manufacturing and space exploration.  The 'Texas Tech Triangle' encompassing Austin, Dallas-Fort Worth, and Houston solidified its national presence.  This wasn't just about big corporations; a thriving startup culture flourished, fueled by angel investors and a willingness to embrace risk.  By 2040, Texas had become a global tech powerhouse, a testament to strategic investment, a supportive environment, and a commitment to inclusive growth.  Its success was not only measured in economic growth but also in the diverse and thriving communities it created.\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai \n",
    "import os \n",
    " \n",
    "#system_instruction\n",
    "system_prompt='''\n",
    "Answer the question in JSON format and nothing else,Do not use code block formatting.\n",
    "'''\n",
    "# test connection for gemini \n",
    "genai.configure(api_key='AIzaSyAsTNGgfFFDV9w5vWUMT9wUc66eO1ls0ps')\n",
    "client = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", system_instruction=system_prompt)\n",
    "response = client.generate_content(\"Write a story about how texas can become a tech hub in the future.\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e053a6ae-2ab3-4bea-9747-cde5e83f68a7",
   "metadata": {},
   "source": [
    "# Create system prompt for the LLM to understand context of the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11723247-1e29-4300-a3f6-61d4d0b2065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create system prompt to extract data in JSON format as required\n",
    " \n",
    "system=\"\"\" You are a network graph maker tasked with analyzing the relationships involving top leaders in the world. Your job is to process the provided context chunk \n",
    "and extract an ontology of terms that represent key entrepreneurs, their associated entities, and all kinds of relationships present in the context.\n",
    " \n",
    "**Guidelines for Extraction:**\n",
    " \n",
    "1. **Identify Key Entrepreneurs and Related Terms**:\n",
    "   - Extract key entrepreneurs and related concepts such as:\n",
    "     - Companies, organizations, or industries they are associated with.\n",
    "     - Collaborators, partners, rivals, or competitors.\n",
    "     - Key innovations, achievements, or milestones.\n",
    "     - Locations, events, or time periods relevant to their actions.\n",
    " \n",
    "2. **Identify Relationships**:\n",
    "   - Extract all types of relationships between entrepreneurs and other entities (or between entities themselves).\n",
    "   - Relationships can include:\n",
    "     - Professional roles or associations.\n",
    "     - Business partnerships, collaborations, or rivalries.\n",
    "     - Innovations or contributions to industries.\n",
    "     - Personal connections or influences.\n",
    "     - Historical events or shared milestones.\n",
    " \n",
    "3. **Define Relationships**:\n",
    "   - Clearly specify the nature of each relationship in simple and concise terms.\n",
    "   - Relationships should convey meaningful connections relevant to the context.\n",
    " \n",
    "**Response Format**:\n",
    "- Provide your output **strictly as a list of JSON objects**. No additional text, descriptions,tags or comments are allowed.\n",
    "- Each object should include the following fields:\n",
    "  - `\"node_1\"`: The first entity in the relationship (can be a person, organization, or concept).\n",
    "  - `\"node_2\"`: The second entity in the relationship.\n",
    "  - `\"edge\"`: A concise sentence describing the relationship between `node_1` and `node_2`.\n",
    " \n",
    "**Example Output**:\n",
    "[\n",
    "   {\n",
    "       \"node_1\": \"Elon Musk\",\n",
    "       \"node_2\": \"SpaceX\",\n",
    "       \"edge\": \"Elon Musk founded SpaceX to revolutionize space exploration.\"\n",
    "   },\n",
    "   {\n",
    "       \"node_1\": \"Steve Jobs\",\n",
    "       \"node_2\": \"Apple Inc.\",\n",
    "       \"edge\": \"Steve Jobs co-founded Apple Inc., a leading tech company.\"\n",
    "   },\n",
    "   {\n",
    "       \"node_1\": \"Mark Zuckerberg\",\n",
    "       \"node_2\": \"Sheryl Sandberg\",\n",
    "       \"edge\": \"Sheryl Sandberg worked closely with Mark Zuckerberg as COO of Facebook.\"\n",
    "   },\n",
    "   {\n",
    "       \"node_1\": \"Jeff Bezos\",\n",
    "       \"node_2\": \"Blue Origin\",\n",
    "       \"edge\": \"Jeff Bezos founded Blue Origin to focus on space exploration.\"\n",
    "   }\n",
    "]\n",
    " \n",
    "**Important Note**:\n",
    "- Always respond exclusively in JSON format. Any deviation from the JSON structure or inclusion of additional text will not be accepted.\n",
    "- Do not use code block formatting like ` ``` `.\n",
    "- Output must be a valid JSON array of objects without any surrounding text.\n",
    " \n",
    "Please provide the context containing information about entrepreneurs and their relationships for analysis.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f421fc-b88c-4a06-92e8-9b8947eaf8e0",
   "metadata": {},
   "source": [
    "# Use LLM’s to extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "535ace59-3074-4cd5-8b69-7206166ccb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "('You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.',)\n",
      "Rate limit hit for model: gemini-1.5-pro\n",
      "Switching to next model: gemini-1.5-flash\n",
      "32\n",
      "('You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.',)\n",
      "Rate limit hit for model: gemini-1.5-flash\n",
      "Switching to next model: gemini-1.5-flash-8b\n",
      "('You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.',)\n",
      "Rate limit hit for model: gemini-1.5-flash-8b\n",
      "Switching to next model: gemini-1.5-pro\n",
      "('You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.',)\n",
      "Rate limit hit for model: gemini-1.5-pro\n",
      "Switching to next model: gemini-1.5-flash\n",
      "47\n",
      "44\n",
      "47\n",
      "14\n",
      "20\n",
      "27\n",
      "12\n",
      "11\n",
      "extracted information in 0:23:17.101645\n",
      "total results: 234\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from itertools import cycle\n",
    "\n",
    "results = []\n",
    "models = [\n",
    "    'gemini-1.5-pro',\n",
    "    'gemini-1.5-flash',\n",
    "    'gemini-1.5-flash-8b',\n",
    "]\n",
    "model_cycle = cycle(models)\n",
    "model_name = next(model_cycle)\n",
    "start_time = datetime.now()\n",
    "\n",
    "for url in url_list:\n",
    "    smaller_doc = extract_data_from_URL(url)\n",
    "    \n",
    "    for doc in smaller_doc[:30]:\n",
    "        while True:\n",
    "            try:\n",
    "                client = genai.GenerativeModel(\n",
    "                    model_name=model_name,\n",
    "                    system_instruction=system,\n",
    "                    generation_config={\"response_mime_type\": 'application/json'}\n",
    "                )\n",
    "                chat_completion = client.generate_content(doc.page_content)\n",
    "                results.append(chat_completion.candidates[0].content.parts[0].text)\n",
    "                break  # Success! Break out of the retry loop.\n",
    "            except Exception as e:\n",
    "                errordata = e.args[0]\n",
    "                print(e.args)\n",
    "                \n",
    "                if 'quota' in errordata or 'exceeded' in errordata or 'limit' in errordata:\n",
    "                    print(f'Rate limit hit for model: {model_name}')\n",
    "                    model_name = next(model_cycle)\n",
    "                    print(f'Switching to next model: {model_name}')\n",
    "                else:\n",
    "                    print(f\"Unhandled error: {e}\")\n",
    "                    break  # Stop retrying for non-rate-limit errors\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f'extracted information in {end_time - start_time}')\n",
    "print(f'total results: {len(results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ac16d9-2887-499c-af25-b009ade30408",
   "metadata": {},
   "source": [
    "# Save the result to JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66689e4b-e721-43c8-afc4-bd7f10a878f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buggy JSON object Extra data: line 1 column 142 (char 141)\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "#print(results)\n",
    "combined_nodes_and_edges=[]\n",
    "for res in results:\n",
    "    try:\n",
    "        combined_nodes_and_edges.extend(json.loads(res)) #convert the string result from LLM to JSON \n",
    "    except Exception as e:\n",
    "        print('buggy JSON object', e)\n",
    " \n",
    "with open('Nodes_and_edges.json','w') as file:\n",
    "    json.dump(combined_nodes_and_edges,file,indent=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6548f15-73ef-4658-a9c9-2da6f89f6df0",
   "metadata": {},
   "source": [
    "# Send JSON data to Gephi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03e259f3-783f-4d55-94f9-bffc6d218138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gephistreamer import graph\n",
    "from gephistreamer import streamer\n",
    "# connect to gephi server\n",
    "# create a stream \n",
    "stream = streamer.Streamer(streamer.GephiWS(hostname=\"localhost\", port=8080, workspace=\"workspace1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d73c24ec-827a-4e18-bab1-ef2e76eeca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the nodes and edges from the json file\n",
    "with open('Nodes_and_edges.json','r') as file:\n",
    "    results=json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89b71276-0bae-4e36-ac50-59821eb4f5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buggy JSON object 'edge' {'node_1': 'Elon Musk', 'node_2': 'rebranded Twitter to X in 2023'}\n",
      "buggy JSON object 'node_2' {'node_1': 'Larry Ellison', 'node': 'Mark Hurd', 'edge': 'Larry Ellison appointed Mark Hurd as CEO of Oracle'}\n"
     ]
    }
   ],
   "source": [
    "# loop through the list of json result and send to Gephi\n",
    "for res in results:\n",
    "    try:        \n",
    "        node_a = graph.Node(res['node_1'],custom_property=1)\n",
    "        node_b = graph.Node(res['node_2'],custom_property=2)\n",
    "        stream.add_node(node_a,node_b)\n",
    "        edge_ab = graph.Edge(node_a,node_b,custom_property=res['edge'])\n",
    "        stream.add_edge(edge_ab)\n",
    " \n",
    "    except Exception as e:\n",
    "        print('buggy JSON object', e,res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028b7db-e19e-4331-bdbd-47a7a528f587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
